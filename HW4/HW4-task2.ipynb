{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Task 1\n",
    "\n",
    "#### Peter Grantcharov (pdg2116), Po-Chieh Liu (pl2441)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "df_train = pd.read_csv('reddit_200k_train.csv', \n",
    "                       encoding = 'latin-1',\n",
    "                       usecols = ['body', 'REMOVED'])\n",
    "# import testing data\n",
    "df_test = pd.read_csv('reddit_200k_test.csv', \n",
    "                      encoding = 'latin-1', \n",
    "                      usecols = ['body', 'REMOVED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into numpy vector\n",
    "y_train = df_train['REMOVED']\n",
    "y_test = df_test['REMOVED']\n",
    "\n",
    "# convert boolean to 0, 1 label\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract body\n",
    "X = df_train['body'].values\n",
    "X_t = df_test['body'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# we dont need ner step in pipe\n",
    "_ = nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_component(X):\n",
    "    # convert everything into spacy nlp\n",
    "    doc = nlp(X[0])\n",
    "    flag = None\n",
    "    count = 1\n",
    "    for tok in doc:\n",
    "        # remove all stopwords, punctuation and symbol\n",
    "        if (tok.is_stop is False and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\"):\n",
    "            if flag is None:\n",
    "                flag = True\n",
    "                vec = tok.vector\n",
    "            else:\n",
    "                # stack all vectors together, and return mean values in the end\n",
    "                vec = np.vstack((vec, tok.vector))\n",
    "    if flag is None:\n",
    "        # if all tokens are in stopwords, just use the raw vector\n",
    "        return doc.vector\n",
    "    return np.mean(vec, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to token, then convert token to vector, then average the vectors\n",
    "X_train = np.apply_along_axis(clean_component, axis = 1, arr = X.reshape(-1,1))\n",
    "X_test = np.apply_along_axis(clean_component, axis = 1, arr = X_t.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed:  1.6min finished\n",
      "/Users/petergrantcharov/anaconda3/envs/appliedml/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "lr_pipe = make_pipeline(LogisticRegression(class_weight= 'balanced',\n",
    "                                           solver = 'lbfgs'))\n",
    "\n",
    "# tune parameter \n",
    "param_grid = {\"logisticregression__C\": np.logspace(-2 , 2, 10)}\n",
    "\n",
    "# gridsearch\n",
    "gs = GridSearchCV(lr_pipe, \n",
    "                  param_grid, \n",
    "                  cv = 3,\n",
    "                  return_train_score=True,\n",
    "                  scoring='f1',\n",
    "                  verbose=2,\n",
    "                  n_jobs=5)\n",
    "#fit\n",
    "_ = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.21544346900318834}\n",
      "0.6013630764011264\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668330743861623\n",
      "0.667726646203207\n"
     ]
    }
   ],
   "source": [
    "gs_train_f1 = f1_score(y_train.reshape(-1,1),\n",
    "                       gs.predict(X_train),\n",
    "                       average='weighted')\n",
    "gs_test_f1 = f1_score(y_test.reshape(-1,1),\n",
    "                      gs.predict(X_test),\n",
    "                      average='weighted')\n",
    "print(gs_train_f1)\n",
    "print(gs_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "As we can see, our score in this task was quite good, although still slightly lower than the score that was achieved with the bag of words approach found in task 1. \n",
    "When considering the model composed in task 1.3, this word embedding was quite a bit lower, although incorporating the word embedding in conjunction with those derived features will likely be more successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appliedML",
   "language": "python",
   "name": "appliedml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
